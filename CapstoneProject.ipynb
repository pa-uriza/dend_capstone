{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I94 DataWarehouse - Data Engineering Capstone Project\n",
    "\n",
    "## Project Summary\n",
    "The aim of this project is to implement the desing and construction of a data warehouse from I94 dataset. Through this, data will be available to perform analysis and run models to support decision making. By combining different data sources, the idea is to enrich current knowledge of the immigration process to make data actionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### 1.1 Scope \n",
    "In this project three data sources are joined to get a better understanding of immigration process. The datasets used will be I94 Immigration Data, U.S. City Demographic Data, and Airport Code Table.\n",
    "\n",
    "Some of the tools used for this project are AWS S3 (for data storage) and pandas and pyspark (for data exploration and processing).\n",
    "\n",
    "#### 1.2 Describe and Gather Data \n",
    "\n",
    "Following are the data sources used for this project:\n",
    "\n",
    "|           Dataset          |                                                                                       Description                                                                                       |                                                                                                                                                    Source                                                                                                                                                    | Format |\n",
    "|:--------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------:|\n",
    "| I94 Immigration Data       | Data from the US National Tourism and Trade Office. Contains international visitor<br>information including demographics and trip specific information as arrival and<br>departure date | [Link](https://classroom.udacity.com/nanodegrees/nd027/parts/dce8f032-1b05-4d57-a30b-d6e41f01e800/modules/c46c3dad-e89f-44a2-9599-b758bfa3a3ba/lessons/b18ab222-552a-432b-aae8-7c52c5e72d37/concepts/7b7f4199-d02b-4684-8e8c-0a58318c62ee#:~:text=in%20the%20workspace.-,This,-is%20where%20the)             | SAS    |\n",
    "| U.S. City Demographic Data | Contains information about the demographics of US cities                                                                                                                                | [Link](https://classroom.udacity.com/nanodegrees/nd027/parts/dce8f032-1b05-4d57-a30b-d6e41f01e800/modules/c46c3dad-e89f-44a2-9599-b758bfa3a3ba/lessons/b18ab222-552a-432b-aae8-7c52c5e72d37/concepts/7b7f4199-d02b-4684-8e8c-0a58318c62ee#:~:text=OpenSoft.%20You%20can%20read%20more%20about%20it-,here,-.) | CSV    |\n",
    "| Airport Code Table         | Contains airport information including location (coordinates and city) and height<br>in feet.                                                                                           | [Link](https://classroom.udacity.com/nanodegrees/nd027/parts/dce8f032-1b05-4d57-a30b-d6e41f01e800/modules/c46c3dad-e89f-44a2-9599-b758bfa3a3ba/lessons/b18ab222-552a-432b-aae8-7c52c5e72d37/concepts/7b7f4199-d02b-4684-8e8c-0a58318c62ee#:~:text=It%20comes%20from-,here,-.)                                | CSV    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### 2.1 Explore the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr  ...  entdepu  matflag  biryear   dtaddto  gender insnum  \\\n",
       "0      1.0      HI  ...      NaN        M   1955.0  07202016       F    NaN   \n",
       "1      1.0      TX  ...      NaN        M   1990.0  10222016       M    NaN   \n",
       "2      1.0      FL  ...      NaN        M   1940.0  07052016       M    NaN   \n",
       "3      1.0      CA  ...      NaN        M   1991.0  10272016       M    NaN   \n",
       "4      3.0      NY  ...      NaN        M   1997.0  07042016       F    NaN   \n",
       "\n",
       "  airline        admnum  fltno  visatype  \n",
       "0      JL  5.658267e+10  00782        WT  \n",
       "1     *GA  9.436200e+10  XBLNG        B2  \n",
       "2      LH  5.578047e+10  00464        WT  \n",
       "3      QR  9.478970e+10  00739        B2  \n",
       "4     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the i94 dataset\n",
    "df_i94 = pd.read_csv('data/immigration_data_sample.csv')\n",
    "df_i94.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([209., 582., 112., 297., 111., 577., 245., 135., 131., 116., 438.,\n",
       "       260., 512., 689., 158., 115., 511., 251., 268., 585., 213., 264.,\n",
       "       509., 324., 696., 117., 687., 129., 528., 123., 258., 691., 130.,\n",
       "       107., 103., 694., 276., 206., 368., 575., 586., 120., 514., 124.,\n",
       "       273., 692., 109., 579., 164., 126., 263., 464., 602., 121., 162.,\n",
       "       274., 690., 207., 104., 525., 105., 518., 343., 504., 576., 272.,\n",
       "       108., 127., 140., 526., 603., 332., 513., 516., 218., 296., 204.,\n",
       "       201., 114., 257., 266., 520., 243., 261., 113., 373., 299., 688.,\n",
       "       141., 350., 340.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.i94res.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'B2', 'CP', 'B1', 'GMT', 'WB', 'F1', 'E2', 'F2', 'M1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.visatype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.cicid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i94port\n",
       "NYC    155\n",
       "MIA    111\n",
       "LOS    106\n",
       "SFR     55\n",
       "CHI     45\n",
       "      ... \n",
       "ONT      1\n",
       "OPF      1\n",
       "OTM      1\n",
       "PEM      1\n",
       "X96      1\n",
       "Length: 70, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.groupby(by='i94port').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i94mode\n",
       "1.0    962\n",
       "3.0     26\n",
       "2.0     10\n",
       "9.0      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.groupby(by='i94mode').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visatype\n",
       "WT     443\n",
       "B2     356\n",
       "WB      91\n",
       "B1      61\n",
       "GMT     27\n",
       "F1      10\n",
       "CP       5\n",
       "E2       3\n",
       "F2       3\n",
       "M1       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.groupby(by='visatype').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore the demographics dataset\n",
    "df_dem = pd.read_csv('data/us-cities-demographics.csv', sep=';')\n",
    "df_dem.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
       "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
       "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34692"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hispanic or Latino', 'White', 'Asian',\n",
       "       'Black or African-American', 'American Indian and Alaska Native'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem['Race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State        Race                             \n",
       "California   White                                137\n",
       "             Hispanic or Latino                   137\n",
       "             Black or African-American            136\n",
       "             Asian                                136\n",
       "             American Indian and Alaska Native    130\n",
       "                                                 ... \n",
       "Hawaii       Asian                                  1\n",
       "             Black or African-American              1\n",
       "             Hispanic or Latino                     1\n",
       "Mississippi  American Indian and Alaska Native      1\n",
       "Hawaii       White                                  1\n",
       "Length: 243, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dem.groupby(by=['State', 'Race']).size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore airport dataset\n",
    "df_airport = pd.read_csv('data/airport-codes_csv.csv')\n",
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country',\n",
       "       'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code',\n",
       "       'coordinates'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'UTK', 'OCA', ..., 'SHE', 'YNJ', 'YKH'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.iata_code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iata_code\n",
       "0      80\n",
       "OHE     3\n",
       "PRI     3\n",
       "BCK     2\n",
       "CLG     2\n",
       "       ..\n",
       "HMA     1\n",
       "HLZ     1\n",
       "HLW     1\n",
       "HLV     1\n",
       "ZZV     1\n",
       "Length: 9042, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.groupby(by='iata_code').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['heliport', 'small_airport', 'closed', 'seaplane_base',\n",
       "       'balloonport', 'medium_airport', 'large_airport'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cleaning Steps\n",
    "\n",
    "1. Parse I94_SAS_Labels_Descriptions.SAS file to get country code, country name mapping\n",
    "\n",
    "2. Join `i94cit` and `i94res` field with country name mapping to get country of citizenship and country of residence of immigrants\n",
    "\n",
    "3. Transform `arrdate`, `depdate` in i94 immigration data from SAS time format to datetime object\n",
    "\n",
    "4. Select only fields according to dimensional model and rename them accordingly\n",
    "\n",
    "5. Drop duplicate values from all tables at defined grain level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "This project follows the 4 steps in Kimball's dimensional modeling methodology:\n",
    "1. **Select the business process:** As discussed in the scope of the project, the main objective is to create a Data Warehouse for support decision making regarding the immigration process. Therefore we focused on the immigration process represented in the data gathered using the i94 form. Through this data model we want to uncover relationships and patterns in migration data collected.\n",
    "\n",
    "2. **Declare the grain:** The grain will be each i94 form record which corresponds to an immigrant filling the form for a specific visit to the US.\n",
    "\n",
    "3. **Identify the dimensions:** We defined a `Dim_Demographics` table containing information of the respondent's destination in the US. We also defined a `Dim_Airports` that contain information of the location of different airports around the world. Finally we defined the table `Dim_Traveler` that contains information of the traveler including gender and year of birth among others.\n",
    "\n",
    "4. **Identify the facts:** The fact table `Fact_Traveler`, contain available facts for this granularity corresponds to the respondent's age (a non-additive fact) and an auxiliar `count` field to perform aggregations.\n",
    "\n",
    "The dimensional model is depicted as follows:\n",
    "\n",
    "![dimensional_model](img/dimensional_model.png)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Raw data is stored in a S3 source bucket from where it will be extracted and transformed to conform a dimensional model for analytics purposes (report generation, statistical modeling, etc.). Transfored dimensional and fact tables are stored into a S3 destination bucket in parquet format. All transformations are performed using python. Data intensive transformations are leveraged using pyspark.\n",
    "\n",
    "![solution](img/solution_stack.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data\n",
    "\n",
    "#### 4.1 Create the data model\n",
    "\n",
    "To build the data model the follwong `bash` is run which performs the aforementioned ETL process\n",
    "\n",
    "``` bash\n",
    "python etl.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "To ensure the pipeline ran as expected, we included some data quality checks:\n",
    " * Assert data schema of every table\n",
    " * Assert non-empty tables after running ETL data pipeline\n",
    " \n",
    "To run them use the `bash` command\n",
    "\n",
    "``` bash\n",
    "python qa.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "**Dim_Demographics**\n",
    "\n",
    "|Field               |Description                                              |\n",
    "|--------------------|---------------------------------------------------------|\n",
    "| state_code         | Two letter code to identify state                       |\n",
    "| state              | US state name                                           |\n",
    "| race               | Ethnicity                                               |\n",
    "| median_age         | Median age                                              |\n",
    "| male_population    | Male population for certain state, city and ethnicity   |\n",
    "| female_population  | Female population for certain state, city and ethnicity |\n",
    "| total_population   | Total population for certain state, city and ethnicity  |\n",
    "\n",
    "**Dim_Airports**\n",
    "\n",
    "| Field        | Description                                                    |\n",
    "|--------------|----------------------------------------------------------------|\n",
    "| iata_code    | International Air Transport Association airport code           |\n",
    "| type         | Type of airport                                                |\n",
    "| name         | Name of the airport                                            |\n",
    "| elevation_ft | Elevation of the airport in feet                               |\n",
    "| continent    | Continent of the airport                                       |\n",
    "| iso_country  | ISO three-letter code for the country where airport is located |\n",
    "| iso_region   | ISO code for the region where airport is located               |\n",
    "| municipality | City or town where airport is located                          |\n",
    "| coordinates  | Longitude and latitude of the airport                          |\n",
    "\n",
    "**Dim_Traveler**\n",
    "\n",
    "| Field       | Description                                |\n",
    "|-------------|--------------------------------------------|\n",
    "| cicid       | Unique immigration identifier for traveler |\n",
    "| arrdate     | Arival date of the traveler                |\n",
    "| depdate     | Departure date of the traveler             |\n",
    "| year        | Year of arrival                            |\n",
    "| month       | Month of arrival                           |\n",
    "| citizenship | Country of citizenship of the traveler     |\n",
    "| residence   | Country of residence of the traveler       |\n",
    "| reason      | Reason of visiting the US                  |\n",
    "| biryear     | Birth year of the traveler                 |\n",
    "| gender      | Gender of the traveler                     |\n",
    "| visatype    | Visa type                                  |\n",
    "\n",
    "**Fact_Traveler**\n",
    "\n",
    "| Field      | Description                                          |\n",
    "|------------|------------------------------------------------------|\n",
    "| cicid      | Unique immigration identifier for traveler           |\n",
    "| state_code | Two letter code to identify state                    |\n",
    "| iata_code  | International Air Transport Association airport code |\n",
    "| age        | Age of the traveler                                  |\n",
    "| count      | Auxiliar variable to count summarize                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "#### Tools and technologies\n",
    "* AWS Simple Storage Service (S3) for raw and processed data storage. Storage on s3 allow to have high volumes of data at relatively low costs. s3 is a good choice for implementing a datalake containing various format data to get staging tables for data modeling\n",
    "* [`pandas`](https://pandas.pydata.org/) for sample EDA as is one of the most popular packages in data science for data wrangling\n",
    "* [`pyspark`](https://spark.apache.org/docs/latest/api/python/) for ETL process for its great capability of massive data processing\n",
    "\n",
    "#### Update frequency\n",
    "* US Demographics should be updated everytime a new report is available. As census data is costly to get, in most countries it is performed once a few years and estimates updated on a yearly basis. If it is the case data should be updated yearly.\n",
    "\n",
    "* Airport codes table will have a very low update frequency as changes in this information is strange. Principally when new airports are built, some new records will appear. It would be prudent to update in a yearly basis.\n",
    "\n",
    "* Immigration data should be updated every month given that raw data seems ti be reported every month. As the granularity for this analysis comes from this source, it is very important to have the most updated data.\n",
    "\n",
    "* As new data is available, tables should have new inserted rows in contrast to replace existing ones.\n",
    "\n",
    "#### Scenario considerations\n",
    "\n",
    "* **The data was increased by 100x:** If data is to big to process in the current ETL pipeline, we could implement massive parallel processing solutions that leverage on Apache Spark in the cloud such as [AWS EMR](https://aws.amazon.com/es/emr/) or [Databricks](https://databricks.com/)\n",
    "\n",
    "* **The data populates a dashboard that must be updated on a daily basis by 7am every day.** An orchestrator solution such as [Airflow](https://airflow.apache.org/) allows to run pipelines using scheduled ETL jobs on certain time basis. It is possible to define a cron expression to run the ETL process daily at 7 am.\n",
    "\n",
    "* **The database needed to be accessed by 100+ people:** In the case a great number of people need to access the data, a Datawarehouse solution could be used to allow multiple simultaneous connections such as [AWS Redshift](https://aws.amazon.com/es/redshift/), also a [Datalakehouse](https://databricks.com/product/data-lakehouse) solution can be used to address this problem.\n",
    "\n",
    "#### Future work\n",
    "\n",
    "* Improve dimensional model including a separate Date dimension to allow more interesting date grouping.\n",
    "* Include weather and temperature dimension.\n",
    "* Implement a Datawarehouse/datalakehouse technology to improve data reading.\n",
    "* Implement data import into the datawarehouse using DBT.\n",
    "* Include an orchestrator (e.g. Airflow) to run the pipeline based on new available data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
